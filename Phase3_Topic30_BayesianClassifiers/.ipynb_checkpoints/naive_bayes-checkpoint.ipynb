{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:200%;\n",
    "           font-family:Arial;letter-spacing:0.5px\">\n",
    "\n",
    "<p width = 20%, style=\"padding: 10px;\n",
    "              color:white;\">\n",
    "Bayes Classifiers (Naive Bayes)\n",
    "              \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Data Science Cohort Live NYC Feb 2022\n",
    "<p>Phase 3: Topic 26</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align = \"right\">\n",
    "<img src=\"Images/flatiron-school-logo.png\" align = \"right\" width=\"200\"/>\n",
    "</div>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Motivating the Bayesian Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far:\n",
    "    \n",
    "- Classified by finding hyperplane:\n",
    "    - maximizing probability on the different classes:\n",
    "    - minimize log loss of sigmoid\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Let's take a second to go through an example to get a feel for how Bayes' Theorem can help us with classification. Specifically about document classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Spam, Spam, Spam, Spam, Spam..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![Many cans of spam](images/wall_of_spam.jpeg)\n",
    "\n",
    "> This is the classic example: detecting email spam!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**The Problem Setup**\n",
    "\n",
    "> We get emails that can be either emails we care about (***ham*** 游냥) or emails we don't care about (***spam*** 游볾). \n",
    ">\n",
    "> We can probably look at the words in the email and get an idea of whether they are spam or not just by observing if they contain red-flag words 游뛀\n",
    "> \n",
    "> We won't always be right, but if we see an email that uses word(s) that are more often associated with spam, then we can feel more confident as labeling that email as spam!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Naive Bayes Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What we gotta do:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Look at spam and not spam (ham) emails\n",
    "2. Identify words that suggest classification\n",
    "3. Determine probability that words occur in each classification\n",
    "4. Profit (classify new emails as \"spam\" or \"ham\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### What's So Great About This?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- We can keep updating our belief based on the emails we detect\n",
    "- Relatively simple\n",
    "- Can expand to multiple words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## The Naive Assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$P(A,B) = P(A\\cap B) = P(A)\\ P(B)$ only if independent \n",
    "\n",
    "In practice, makes sense & is usually pretty good assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## The Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's say the word that occurs is \"cash\":\n",
    "\n",
    "$$ P(游볾 | \"cash\") = \\frac{P(\"cash\" | 游볾)P(游볾)}{P(\"cash\")}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### What Parts Can We Find?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- $P(\"cash\")$\n",
    "    * That's just the probability of finding the word \"cash\"! Frequency of the word!\n",
    "- $P(游볾)$\n",
    "    * Well, we start with some data (*prior knowledge*): The frequency of the spam occurring!\n",
    "- $P(\"cash\" | 游볾)$\n",
    "    * How frequently \"cash\" is used in known spam emails. Count the frequency across all spam emails\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Calculating the Probability That Our Email Is Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Let's just say 2% of all emails have the word \"cash\" in them\n",
    "p_cash = 0.02\n",
    "\n",
    "# We normally would measure this from our data, but we'll take \n",
    "# it that 10% of all emails we collected were spam\n",
    "p_spam = 0.10\n",
    "\n",
    "# 12% of all spam emails have the word \"cash\"\n",
    "p_cash_given_its_spam = 0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p_spam_given_cash = p_cash_given_its_spam * p_spam / p_cash\n",
    "print(f'If the email has the word \"cash\" in it, there is a \\\n",
    "{p_spam_given_cash*100}% chance the email is spam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Check it**: Does this make sense? <br>\n",
    "> Suppose I had 250 total emails.\n",
    "> - How many should I expect to have the word 'cash' in them? (Ans. 5)\n",
    "> - How many should I expect to be spam? (Ans. 25)\n",
    "> - How many *of the spam emails* should I expect to have the word 'cash' in them? (Ans. 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Extending It With Multiple Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> With more words, the more certain we can be if it is/isn't spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Spam:\n",
    "\n",
    "$$ P(游볾\\ |\"buy\",\\ \"cash\") \\propto P(\"buy\",\\ \"cash\"|\\ 游볾)\\ P(游볾)$$\n",
    "\n",
    "\n",
    "But because of independence: \n",
    "    \n",
    "$$ P(\"buy\",\\ \"cash\"|\\ 游볾) = P(\"buy\"|\\ 游볾)\\ P(\"cash\"|\\ 游볾)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Normalize by dividing!\n",
    "\n",
    "$$\n",
    "P(游볾\\ |\"buy\",\\ \"cash\")  =\n",
    "    \\frac\n",
    "        {P(\"buy\"|\\ 游볾)P(\"cash\"|\\ 游볾)\\ P(游볾)}\n",
    "        {P(\"buy\"|\\ 游볾)P(\"cash\"|\\ 游볾)\\ P(游볾) + P(\"buy\"|\\ 游냥)P(\"cash\"|\\ 游냥)\\ P(游냥)}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Note:** If we wanted to find the most probable class (especially useful for *multiclass*), we find the maximum numerator for the given criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Naive Bayes Modeling Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Using Bayes's Theorem for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's recall Bayes's Theorem:\n",
    "\n",
    "$\\large P(h|e) = \\frac{P(h)P(e|h)}{P(e)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Does this look like a classification problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Suppose we have three competing hypotheses $\\{h_1, h_2, h_3\\}$ that would explain our evidence $e$.\n",
    "    - Then we could use Bayes's Theorem to calculate the posterior probabilities for each of these three:\n",
    "        - $P(h_1|e) = \\frac{P(h_1)P(e|h_1)}{P(e)}$\n",
    "        - $P(h_2|e) = \\frac{P(h_2)P(e|h_2)}{P(e)}$\n",
    "        - $P(h_3|e) = \\frac{P(h_3)P(e|h_3)}{P(e)}$\n",
    "        \n",
    "- Suppose the evidence is a collection of elephant heights.\n",
    "- Suppose each of the three hypotheses claims that the elephant whose measurements we have belongs to one of the three extant elephant species (*L. africana*, *L. cyclotis*, and *E. maximus*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In that case the left-hand sides of these equations represent the probability that the elephant in question belongs to a given species.\n",
    "\n",
    "If we think of the species as our target, then **this is just an ordinary classification problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What about the right-hand sides of the equations? **These other probabilities we can calculate from our dataset.**\n",
    "\n",
    "- The priors can simply be taken to be the percentages of the different classes in the dataset.\n",
    "- What about the likelihoods?\n",
    "    - If the relevant features are **categorical**, we can simply count the numbers of each category in the dataset. For example, if the features are whether the elephant has tusks or not, then, to calculate the likelihoods, we'll just count the tusked and non-tuksed elephants per species.\n",
    "    - If the relevant features are **numerical**, we'll have to do something else. A good way of proceeding is to rely on (presumed) underlying distributions of the data. [Here](https://medium.com/analytics-vidhya/use-naive-bayes-algorithm-for-categorical-and-numerical-data-classification-935d90ab273f) is an example of using the normal distribution to calculate likelihoods. We'll follow this idea below for our elephant data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Elephant Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Suppose we have a dataset that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "elephs = pd.read_csv('data/elephants.csv', usecols=['height (cm)',\n",
    "                                                   'species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "elephs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elephs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.kdeplot(data=elephs[elephs['species'] == 'maximus']['height (cm)'],\n",
    "            ax=ax, label='maximus')\n",
    "sns.kdeplot(data=elephs[elephs['species'] == 'africana']['height (cm)'],\n",
    "            ax=ax, label='africana')\n",
    "sns.kdeplot(data=elephs[elephs['species'] == 'cyclotis']['height (cm)'],\n",
    "            ax=ax, label='cyclotis')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Naive Bayes by Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Suppose we want to make prediction of species for some new elephant whose height we've just recorded. We'll suppose the new elephant has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "new_ht = 263"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What we want to calculate is the mean and standard deviation for height for each elephant species. We'll use these to calculate the relevant likelihoods.\n",
    "\n",
    "So:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max_stats = elephs[elephs['species'] == 'maximus'].describe().loc[['mean', 'std'], :]\n",
    "max_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cyc_stats = elephs[elephs['species'] == 'cyclotis'].describe().loc[['mean', 'std'], :]\n",
    "cyc_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "afr_stats = elephs[elephs['species'] == 'africana'].describe().loc[['mean', 'std'], :]\n",
    "afr_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "elephs['species'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Calculation of Likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll use the PDFs of the normal distributions with the discovered means and standard deviations to calculate likelihoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stats.norm(loc=max_stats['height (cm)'][0],\n",
    "           scale=max_stats['height (cm)'][1]).pdf(263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stats.norm(loc=cyc_stats['height (cm)'][0],\n",
    "          scale=cyc_stats['height (cm)'][1]).pdf(263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stats.norm(loc=afr_stats['height (cm)'][0],\n",
    "          scale=afr_stats['height (cm)'][1]).pdf(263)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What we have just calculated are (approximations of) the likelihoods, i.e.:\n",
    "\n",
    "- $P(height=263 | species=maximus) = 2.04\\%$\n",
    "- $P(height=263 | species=cyclotis) = 1.50\\%$\n",
    "- $P(height=263 | species=africana) = 0.90\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "(Notice that they do NOT sum to 1!) But what we'd really like to know are the posteriors. I.e. what are:\n",
    "\n",
    "- $P(species=maximus | height=263)$?\n",
    "- $P(species=cyclotis | height=263)$?\n",
    "- $P(species=africana | height=263)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since we have equal numbers of each species, every prior is equal to $\\frac{1}{3}$. Thus we can calculate the probability of the evidence:\n",
    "\n",
    "$P(height=263) = \\frac{1}{3}(0.0204 + 0.0150 + 0.0090) = 0.0148$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And therefore calculate the posteriors using Bayes's Theorem:\n",
    "\n",
    "- $P(species=maximus | height=263) = \\frac{1}{3}\\frac{0.0204}{0.0148} = 45.9\\%$;\n",
    "- $P(species=cyclotis | height=263) = \\frac{1}{3}\\frac{0.0150}{0.0148} = 33.8\\%$;\n",
    "- $P(species=africana | height=263) = \\frac{1}{3}\\frac{0.0090}{0.0148} = 20.3\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Bayes's Theorem shows us that the largest posterior belongs to the *maximus* species. (Note also that, since the priors are all the same, the largest posterior will necessarily belong to the species with the largest likelihood!)\n",
    "\n",
    "Therefore, the *maximus* species will be our prediction for an elephant of this height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### More Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In fact, we also have elephant *weight* data available in addition to their heights. To accommodate multiple features we can make use of **multivariate normal** distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![multivariate-normal](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8e/MultivariateNormal.png/440px-MultivariateNormal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### What's \"Naive\" about This?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For multiple predictors, we make the simplifying assumption that **our predictors are probablistically independent**. This will often be unrealistic, but it simplifies our calculations a great deal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "elephants = pd.read_csv('data/elephants.csv',\n",
    "                       usecols=['height (cm)', 'weight (lbs)', 'species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "elephants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "maximus = elephants[elephants['species'] == 'maximus']\n",
    "cyclotis = elephants[elephants['species'] == 'cyclotis']\n",
    "africana = elephants[elephants['species'] == 'africana']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Suppose our new elephant with a height of 263 cm also has a weight of 7009 lbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximus.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "likeli_max = stats.multivariate_normal(mean=maximus.mean(),\n",
    "                          cov=maximus.cov()).pdf([263, 7009])\n",
    "likeli_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "likeli_cyc = stats.multivariate_normal(mean=cyclotis.mean(),\n",
    "                         cov=cyclotis.cov()).pdf([263, 7009])\n",
    "likeli_cyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "likeli_afr = stats.multivariate_normal(mean=africana.mean(),\n",
    "                         cov=africana.cov()).pdf([263, 7009])\n",
    "likeli_afr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "post_max = likeli_max / sum([likeli_max, likeli_cyc, likeli_afr])\n",
    "post_cyc = likeli_cyc / sum([likeli_max, likeli_cyc, likeli_afr])\n",
    "post_afr = likeli_afr / sum([likeli_max, likeli_cyc, likeli_afr])\n",
    "\n",
    "print(post_max)\n",
    "print(post_cyc)\n",
    "print(post_afr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### [`GaussianNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gnb = GaussianNB(priors=[1/3, 1/3, 1/3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = elephants.drop('species', axis=1)\n",
    "y = elephants['species']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gnb.predict_proba(np.array([263, 7009]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(gnb, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Comma Survey Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "commas = pd.read_csv('data/comma-survey.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "commas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "commas.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll go ahead and drop the NaNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "commas = commas.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "commas.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The first question on the survey was about the Oxford comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commas['In your opinion, which sentence is more gramatically correct?'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Personally, I like the Oxford comma, since it can help eliminate ambiguities, such as:\n",
    "\n",
    "\"This book is dedicated to my parents, Ayn Rand, and God\" <br/> vs. <br/>\n",
    "\"This book is dedicated to my parents, Ayn Rand and God\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's see how a Naive Bayes model would make a prediction here. We'll think of the comma preference as our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "commas['Age'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Suppose we want to make a prediction about Oxford comma usage for a new person who falls into the **45-60 age group**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Calculating Priors and Likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The following code makes a table of values that count up the number of survey respondents who fall into each of eight bins (the four age groups and the two answers to the first comma question). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "table = np.zeros((2, 4))\n",
    "\n",
    "for idx, value in enumerate(commas['Age'].value_counts().index):\n",
    "    table[0, idx] = len(commas[(commas['In your opinion, which sentence is '\\\n",
    "                                       'more gramatically correct?'] ==\\\n",
    "                                        'It\\'s important for a person to be '\\\n",
    "                                'honest, kind, and loyal.') & (commas['Age'] == value)])\n",
    "    table[1, idx] = len(commas[(commas['In your opinion, which sentence is '\\\n",
    "                                       'more gramatically correct?'] ==\\\n",
    "                                        'It\\'s important for a person to be '\\\n",
    "                                'honest, kind and loyal.') & (commas['Age'] == value)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(table, columns=['Age45-60',\n",
    "                            'Age>60',\n",
    "                            'Age30-44',\n",
    "                            'Age18-29'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['Oxford'] = [True, False]\n",
    "df = df[['Age>60', 'Age45-60', 'Age30-44', 'Age18-29', 'Oxford']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since all we have is a single categorical feature here we can just read our likelihoods and priors right off of this table:\n",
    "\n",
    "Likelihoods:\n",
    "\n",
    "- Age45-60:\n",
    "    - P(Age45-60 | Oxford=True) = $\\frac{123}{470} = 0.2617$;\n",
    "    - P(Age45-60 | Oxford=False) = $\\frac{125}{355} = 0.3521$.\n",
    "\n",
    "Priors:\n",
    "\n",
    "- P(Oxford=True) = $\\frac{470}{825} = 0.5697$;\n",
    "- P(Oxford=False) = $\\frac{355}{825} = 0.4303$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Calculating Posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First we'll calculate the probability of the evidence:\n",
    "\n",
    "$$\\begin{align} \n",
    "    P(Age45-60) &= P(Age45-60 | Oxford=True) \\cdot P(Oxford=True) \\\\\n",
    "                & \\hspace{1cm} + P(Age45-60 | Oxford=False) \\cdot P(Oxford=False)\\\\ \n",
    "                &= 0.2617 \\cdot 0.5697 + 0.3521 \\cdot 0.4303 \\\\\n",
    "                &= 0.3006\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This calculation should also yield P(e):\n",
    "# It's the proportion of 45-60-yr.-olds\n",
    "# in the data.\n",
    "\n",
    "(123+125) / 825"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now use Bayes's Theorem to calculate the posteriors:\n",
    "\n",
    "$$\\begin{align}\n",
    "P(Oxford=True | Age45-60) &= P(Oxford=True) \\cdot P(Age45-60 | Oxford=True) / P(Age45-60) \\\\\n",
    "                          &= 0.5697 \\cdot 0.2617 / 0.3006 \\\\\n",
    "                          &= 0.4960 \\\\\n",
    "                          \\\\\n",
    "P(Oxford=False | Age45-60) &= P(Oxford=False) \\cdot P(Age45-60 | Oxford=False) / P(Age45-60) \\\\ \n",
    "                          &= 0.4303 \\cdot 0.3521 / 0.3006 \\\\\n",
    "                          &= 0.5040\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Close! But our prediction for someone in the 45-60 age group will be that they **do not** favor the Oxford comma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Comparison with [`MultinomialNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "comma_model = MultinomialNB()\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(commas['Age'].values.reshape(-1, 1))\n",
    "\n",
    "X = ohe.transform(commas['Age'].values.reshape(-1, 1)).todense()\n",
    "y = commas['In your opinion, which sentence is more gramatically correct?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "comma_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "comma_model.predict_proba(np.array([0, 0, 1, 0]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "TOC",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "221px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
