{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Objectives</a></span></li><li><span><a href=\"#Decision-Trees-at-a-High-Level\" data-toc-modified-id=\"Decision-Trees-at-a-High-Level-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Decision Trees at a High Level</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simple-Example-of-a-Decision-Tree\" data-toc-modified-id=\"Simple-Example-of-a-Decision-Tree-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Simple Example of a Decision Tree</a></span><ul class=\"toc-item\"><li><span><a href=\"#Picturing-Decisions-as-a-Tree\" data-toc-modified-id=\"Picturing-Decisions-as-a-Tree-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Picturing Decisions as a Tree</a></span></li></ul></li><li><span><a href=\"#Overview-of-Algorithm's-Steps\" data-toc-modified-id=\"Overview-of-Algorithm's-Steps-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Overview of Algorithm's Steps</a></span></li></ul></li><li><span><a href=\"#Entropy/Information-Gain-and-Gini\" data-toc-modified-id=\"Entropy/Information-Gain-and-Gini-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Entropy/Information Gain and Gini</a></span><ul class=\"toc-item\"><li><span><a href=\"#Entropy\" data-toc-modified-id=\"Entropy-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Entropy</a></span><ul class=\"toc-item\"><li><span><a href=\"#Information-Gain\" data-toc-modified-id=\"Information-Gain-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Information Gain</a></span></li></ul></li><li><span><a href=\"#Gini-Impurity\" data-toc-modified-id=\"Gini-Impurity-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Gini Impurity</a></span></li></ul></li><li><span><a href=\"#With-sklearn\" data-toc-modified-id=\"With-sklearn-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>With <code>sklearn</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Setting-up-Data\" data-toc-modified-id=\"Setting-up-Data-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Setting up Data</a></span></li><li><span><a href=\"#Training-the-Model\" data-toc-modified-id=\"Training-the-Model-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Training the Model</a></span></li><li><span><a href=\"#Predictions-and-Evaluation\" data-toc-modified-id=\"Predictions-and-Evaluation-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Predictions and Evaluation</a></span></li></ul></li><li><span><a href=\"#Important-Terminology-Related-to-Decision-Trees\" data-toc-modified-id=\"Important-Terminology-Related-to-Decision-Trees-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Important Terminology Related to Decision Trees</a></span></li><li><span><a href=\"#Challenges-with-Decision-Trees\" data-toc-modified-id=\"Challenges-with-Decision-Trees-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Challenges with Decision Trees</a></span><ul class=\"toc-item\"><li><span><a href=\"#Decision-Trees-are-Prone-to-Overfitting\" data-toc-modified-id=\"Decision-Trees-are-Prone-to-Overfitting-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Decision Trees are Prone to Overfitting</a></span></li><li><span><a href=\"#Bias-Variance-with-Decision-Trees\" data-toc-modified-id=\"Bias-Variance-with-Decision-Trees-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Bias-Variance with Decision Trees</a></span><ul class=\"toc-item\"><li><span><a href=\"#Stopping-Criterion---Pruning-Parameters\" data-toc-modified-id=\"Stopping-Criterion---Pruning-Parameters-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Stopping Criterion - Pruning Parameters</a></span></li></ul></li></ul></li><li><span><a href=\"#Feature-Importances\" data-toc-modified-id=\"Feature-Importances-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Feature Importances</a></span></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Conclusions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pros\" data-toc-modified-id=\"Pros-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Pros</a></span></li><li><span><a href=\"#Cons\" data-toc-modified-id=\"Cons-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Cons</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, plot_roc_curve, plot_confusion_matrix\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Describe the decision tree modeling algorithm\n",
    "- Use attribute selection methods to build different trees\n",
    "- Explain the pros and cons of decision trees\n",
    "- Interpret the feature importances of a fitted model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Decision Trees at a High Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Decision trees** are a supervised learning model that uses past data to form a graph/pathway which leads to the model making _decisions_ on its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I like to think of decision trees as a bunch of forks in the road."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a title=\"Jonathan Billinger / Fork in the road\" href=\"https://commons.wikimedia.org/wiki/File:Fork_in_the_road_-_geograph.org.uk_-_1355424.jpg\"><img width=\"512\" alt=\"Fork in the road - geograph.org.uk - 1355424\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/71/Fork_in_the_road_-_geograph.org.uk_-_1355424.jpg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Every time we make a decision, we split up, or *partition*, the data based on the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Simple Example of a Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's say we have this set of data:\n",
    "\n",
    "Work Status |  Age  | Favorite Website\n",
    "------------|-------|-------------------------\n",
    " Student    | Young | A\n",
    " Working    | Young | B\n",
    " Working    | Old   | C\n",
    " Working    | Young | B\n",
    " Student    | Young | A\n",
    " Student    | Young | A\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This can help us answer a couple questions:\n",
    "\n",
    "- If someone is a young worker, what website do we recommend?\n",
    "- If someone is an old worker, what website then?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Picturing Decisions as a Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](img/simple_decision_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note our tree would look different depending on where we made our decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Overview of Algorithm's Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Here's a great visual of a decision tree  http://www.r2d3.us/visual-intro-to-machine-learning-part-1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Organize data features and target\n",
    "2. Make a *decision* (a split) based on some *metric* using the features\n",
    "    * Data are split into partitions via *branches*\n",
    "3. Continue on with each partition, and do more splits for each using the features in that partition\n",
    "4. Keep doing that until a **stopping condition** is hit\n",
    "    - Number of data points in a final partition\n",
    "    - Layers deep\n",
    "5. To make predictions, run through the decision nodes (the forks in the road)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we have to determine what metric we use to make our split/decision!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Entropy/Information Gain and Gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The goal is to have our ultimate classes be fully \"ordered\" (for a binary dependent variable, we'd have the 1's in one group and the 0's in the other)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](img/information_gain_split.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "One way to assess the value of a split is to measure how *disordered* our groups are, and there is a notion of *entropy* that measures precisely this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The entropy of the whole dataset is given by:\n",
    "\n",
    "$\\large E = -\\Sigma^n_i p_i\\log_2(p_i)$,\n",
    "\n",
    "where $p_i$ is the probability of belonging to the $i$th group, where $n$ is the number of groups (i.e. target values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Entropy will always be between 0 and 1. The closer to 1, the more disordered your group.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To calculate the entropy of a *split*, we're going to want to calculate the entropy of each of the groups made by the split, and then calculate a weighted average of those groups' entropies––weighted, that is, by the size of the groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For a given split, the **information gain** is simply the entropy of the parent group less the entropy of the split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> We can essentially say **information gain** is the **_difference_** of the **parent's entropy** and the **_average_** of the **children's entropy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For a given parent, then, we maximize our model's performance by *minimizing* the split's entropy.\n",
    "\n",
    "What we'd like to do then is:\n",
    "\n",
    "1. Look at the entropies of all possible splits\n",
    "2. Choose the split with the lowest entropy\n",
    "\n",
    "In practice there are far too many splits for it to be practical for a person to calculate all these different entropies ...\n",
    "\n",
    "... but we can make computers do these calculations for us!\n",
    "\n",
    "Moreover, we can **iterate** this algorithm on the resultant groups until we reach pure groups!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Question**: Are we in fact guaranteed, proceeding in this way, to reach pure groups, no matter what our data looks like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Observation**: This algorithm looks for the best split **locally**. There is no regard for how an overall tree might look. That's what makes this algorithm ***greedy***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Gini Impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "An alternative metric to entropy comes from the work of Corrado Gini. The Gini Impurity is defined as:\n",
    "\n",
    "$\\large G = 1 - \\Sigma_ip_i^2$, or, equivalently, $\\large G = \\Sigma_ip_i(1-p_i)$.\n",
    "\n",
    "where, again, $p_i$ is the probability of belonging to the $i$th group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Gini Impurity will always be between 0 and 0.5. The closer to 0.5, the more disordered your group.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# With `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Setting up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iris = load_iris() \n",
    "print(iris.target_names)\n",
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# petal length and width features\n",
    "feature_used = iris.feature_names[2:]\n",
    "X = iris.data[:, 2:] \n",
    "y = iris.target \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first ten rows of X\n",
    "X[:10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last ten rows of X\n",
    "X[-10:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Check out the [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "\n",
    "tree_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Predictions and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred = tree_clf.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_pred) * 100\n",
    "print(\"Accuracy: {0}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(tree_clf, X_test, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "plot_tree(tree_clf, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Important Terminology Related to Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- **Root Node:** Represents entire population or sample.\n",
    "- **Decision Node:** Node that is split.\n",
    "- **Leaf/ Terminal Node:** Node with no children.\n",
    "- **Pruning:** Removing nodes.\n",
    "- **Branch / Sub-Tree:** A sub-section of a decision tree.\n",
    "- **Parent and Child Node:** A node divided into sub-nodes is the parent; the sub-nodes are its children.\n",
    "\n",
    "<img src='./images/decision_leaf.webp' width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Challenges with Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Decision Trees are Prone to Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Using more data features this time\n",
    "feature_used = iris.feature_names[:]\n",
    "X = iris.data[:, :]\n",
    "y = iris.target \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Allow it to run the full default hyperparameters\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Accuracy on training data\n",
    "tree_clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Accuracy on test data\n",
    "tree_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Bias-Variance with Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The CART algorithm will repeatedly partition data into smaller and smaller subsets until those final subsets are homogeneous in terms of the outcome variable. In practice this often means that the final subsets (known as the leaves of the tree) each consist of only one or a few data points. \n",
    "\n",
    "This tends to result in low-bias, high variance models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Allow it to run the full default hyperparameters\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# Accuracy on training data & test data\n",
    "print('Training:', tree_clf.score(X_train, y_train))\n",
    "print('Testing:', tree_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Stopping Criterion - Pruning Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The recursive binary splitting procedure described above needs to know when to stop splitting as it works its way down the tree with the training data.\n",
    "\n",
    "**min_samples_leaf:**  The most common stopping procedure is to use a minimum count on the number of training instances assigned to each leaf node. If the count is less than some minimum then the split is not accepted and the node is taken as a final leaf node.\n",
    "\n",
    "**max_leaf_nodes:** \n",
    "Reduce the number of leaf nodes.\n",
    "\n",
    "**max_depth:**\n",
    "Reduce the depth of the tree to build a generalized tree.\n",
    "\n",
    "**min_impurity_decrease:**\n",
    "A node will split if the impurity decrease in the split is above the threshold, otherwise it will be a leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Stop it from running too long\n",
    "tree_clf = DecisionTreeClassifier(min_impurity_decrease=0.3, random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# Accuracy on training data & test data\n",
    "print('Training:', tree_clf.score(X_train, y_train))\n",
    "print('Testing:', tree_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The fitted tree has an attribute called `ct.feature_importances_`. What does this mean? Roughly, the importance (or \"Gini importance\") of a feature is a sort of weighted average of the impurity decrease at internal nodes that make use of the feature. The weighting comes from the number of samples that depend on the relevant nodes.\n",
    "\n",
    "> The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance. See [`sklearn`'s documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.feature_importances_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "dt.fit(X, y)\n",
    "\n",
    "for fi, feature in zip(dt.feature_importances_, feature_used):\n",
    "    print(fi, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "More on feature importances [here](https://towardsdatascience.com/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- The decision tree is a \"white-box\" type of ML algorithm. It shares internal decision-making logic, which is not available in the black-box type of algorithms such as Neural Network.\n",
    "- Its training time is faster compared to other algorithms such as neural networks.\n",
    "- The decision tree is a non-parametric method, which does not depend upon probability distribution assumptions.\n",
    "- Decision trees can handle high-dimensional data with good accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Pros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Easy to interpret and visualize\n",
    "- Can easily capture non-linear patterns\n",
    "- Require little data preprocessing from the user (no need to normalize data)\n",
    "- Can be used for feature engineering such as variable selection and predicting missing values \n",
    "- Make no assumptions about distribution because its non-parametric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Sensitive to noisy data (overfit)\n",
    "- Trouble with imbalanced datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "280.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
