{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:200%;\n",
    "           font-family:Arial;letter-spacing:0.5px\">\n",
    "\n",
    "<p width = 20%, style=\"padding: 10px;\n",
    "              color:white;\">\n",
    "Natural Language Processing: Intro and Preprocessing\n",
    "              \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Data Science Cohort Live NYC Feb 2022\n",
    "<p>Phase 4: Topic 35</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align = \"right\">\n",
    "<img src=\"Images/flatiron-school-logo.png\" align = \"right\" width=\"200\"/>\n",
    "</div>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Use this to install nltk if needed\n",
    "# !pip install nltk\n",
    "# !conda install -c anaconda nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this to download the stopwords if you haven't already - only ever needs to be run once\n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Natural Language Processing (NLP)\n",
    "\n",
    "- Machine learning tasks with unstructured free language text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Supervised learning: training on labeled free text documents\n",
    "- Build document classifiers\n",
    "\n",
    "<center><img src = \"Images/spamvsham.png\" />\n",
    "Spam filtration </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<img src = \"Images/doc_classification.jpg\" />\n",
    "Document management systems for your business\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Using free text as input in regression.\n",
    "    - e.g., free text reviews to predict restaurant quality 0-10\n",
    "    - sentiment analysis (extremely displeased to ecstatic)\n",
    "    \n",
    "<img src = \"Images/anton_ego.jpg\" width = 450/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = \"Images/ego_quote.jpg\" width = 450/>\n",
    "<center> Our algorithm predicts a 9.8 for Gusteau's. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Based on text:\n",
    "- Algorithm predicts Anton was extremely pleased.\n",
    "\n",
    "<center><img src = \"Images/sentiment_analysis.jpg\" > Sentiment Analysis</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Unsupervised Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Topic modeling\n",
    "    - learn topics from a collection of documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"Images/topicmodels.png\" width = 900 ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Many, many more types of NLP tasks.\n",
    "- Just named a few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Need to represent information in free text in a form useable by an ML model:\n",
    "- i.e. vectorize/structure information inside body of documents\n",
    "- create numeric representations of words, sentences, documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Simple example: count vectorizer\n",
    "<img src = \"Images/vectorchart.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Processing texting is multistep:\n",
    "- Text pre-processing\n",
    "- Feature extraction (vectorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A simple NLP workflow:\n",
    "\n",
    "<img src = \"Images/text_feature_pipe.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Many types of vectorization schemes exist that can be trained:\n",
    "\n",
    "- But first: text data must be preprocessed.\n",
    "- This is the first phase in the NLP pipeline\n",
    "- **Essential**: helps learning effective vector representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Text Preprocessing\n",
    "1. **Tokenization**\n",
    "2. Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Tokenizing: cutting text into small semantic subunits (tokens).\n",
    "<img src = \"Images/tokenization.webp\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Tokenization: language-specific splitting/contraction rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Many NLP packages with excellent tokenizers (among other things):\n",
    "- nltk\n",
    "- spaCy\n",
    "- gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Will use nltk: The Natural Language Toolkit\n",
    "    \n",
    "<center><img src = \"Images/nltk_logo.png\" width = 250></center>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk # the natural language toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to downlod punkt to access better tokenization rules\n",
    "# word_tokenize won't work without it\n",
    "nltk.download('punkt') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize # nltk's gold standard word tokenizer\n",
    "from nltk.tokenize import sent_tokenize # nltk's sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "satire_df = pd.read_csv('data/satire_nosatire.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Predict whether an article is satire or real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Noting that the resignation of James Mattis as...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Desperate to unwind after months of nonstop wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nearly halfway through his presidential term, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Attempting to make amends for gross abuses of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decrying the Senate’s resolution blaming the c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  target\n",
       "0  Noting that the resignation of James Mattis as...       1\n",
       "1  Desperate to unwind after months of nonstop wo...       1\n",
       "2  Nearly halfway through his presidential term, ...       1\n",
       "3  Attempting to make amends for gross abuses of ...       1\n",
       "4  Decrying the Senate’s resolution blaming the c...       1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "satire_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   body    1000 non-null   object\n",
      " 1   target  1000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 15.8+ KB\n"
     ]
    }
   ],
   "source": [
    "satire_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Noting that the resignation of James Mattis as Secretary of Defense marked the ouster of the third top administration official in less than three weeks, a worried populace told reporters Friday that it was unsure how many former Trump staffers it could safely reabsorb. “Jesus, we can’t just take back these assholes all at once—we need time to process one before we get the next,” said 53-year-old Gregory Birch of Naperville, IL echoing the concerns of 323 million Americans in also noting that the country was only now truly beginning to reintegrate former national security advisor Michael Flynn. “This is just not sustainable. I’d say we can handle maybe one or two more former members of Trump’s inner circle over the remainder of the year, but that’s it. This country has its limits.” The U.S. populace confirmed that they could not handle all of these pieces of shit trying to rejoin society at once.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_doc = satire_df['body'].iloc[0]\n",
    "first_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see what word tokenizer does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Noting', 'that', 'the', 'resignation', 'of', 'James', 'Mattis', 'as', 'Secretary', 'of', 'Defense', 'marked', 'the', 'ouster', 'of', 'the', 'third', 'top', 'administration', 'official', 'in', 'less', 'than', 'three', 'weeks', ',', 'a', 'worried', 'populace', 'told', 'reporters', 'Friday', 'that', 'it', 'was', 'unsure', 'how', 'many', 'former', 'Trump', 'staffers', 'it', 'could', 'safely', 'reabsorb', '.', '“', 'Jesus', ',', 'we', 'can', '’', 't', 'just', 'take', 'back', 'these', 'assholes', 'all', 'at', 'once—we', 'need', 'time', 'to', 'process', 'one', 'before', 'we', 'get', 'the', 'next', ',', '”', 'said', '53-year-old', 'Gregory', 'Birch', 'of', 'Naperville', ',', 'IL', 'echoing', 'the', 'concerns', 'of', '323', 'million', 'Americans', 'in', 'also', 'noting', 'that', 'the', 'country', 'was', 'only', 'now', 'truly', 'beginning', 'to', 'reintegrate', 'former', 'national', 'security', 'advisor', 'Michael', 'Flynn', '.', '“', 'This', 'is', 'just', 'not', 'sustainable', '.', 'I', '’', 'd', 'say', 'we', 'can', 'handle', 'maybe', 'one', 'or', 'two', 'more', 'former', 'members', 'of', 'Trump', '’', 's', 'inner', 'circle', 'over', 'the', 'remainder', 'of', 'the', 'year', ',', 'but', 'that', '’', 's', 'it', '.', 'This', 'country', 'has', 'its', 'limits.', '”', 'The', 'U.S.', 'populace', 'confirmed', 'that', 'they', 'could', 'not', 'handle', 'all', 'of', 'these', 'pieces', 'of', 'shit', 'trying', 'to', 'rejoin', 'society', 'at', 'once', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(first_doc, language='english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Deals with splitting on whitespace, punctuation, and contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Noting that the resignation of James Mattis as Secretary of Defense marked the ouster of the third top administration official in less than three weeks, a worried populace told reporters Friday that it was unsure how many former Trump staffers it could safely reabsorb. “Jesus, we can’t just take back these assholes all at once—we need time to process one before we get the next,” said 53-year-old Gregory Birch of Naperville, IL echoing the concerns of 323 million Americans in also noting that the country was only now truly beginning to reintegrate former national security advisor Michael Flynn. “This is just not sustainable. I’d say we can handle maybe one or two more former members of Trump’s inner circle over the remainder of the year, but that’s it. This country has its limits.” The U.S. populace confirmed that they could not handle all of these pieces of shit trying to rejoin society at once.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are other more powerful tokenizers that can be dialect specific.\n",
    "\n",
    "Can explore this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The sentence tokenizer\n",
    "- sometimes want to chunk sentences before doing word tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Noting that the resignation of James Mattis as Secretary of Defense marked the ouster of the third top administration official in less than three weeks, a worried populace told reporters Friday that it was unsure how many former Trump staffers it could safely reabsorb.',\n",
       " '“Jesus, we can’t just take back these assholes all at once—we need time to process one before we get the next,” said 53-year-old Gregory Birch of Naperville, IL echoing the concerns of 323 million Americans in also noting that the country was only now truly beginning to reintegrate former national security advisor Michael Flynn.',\n",
       " '“This is just not sustainable.',\n",
       " 'I’d say we can handle maybe one or two more former members of Trump’s inner circle over the remainder of the year, but that’s it.',\n",
       " 'This country has its limits.” The U.S. populace confirmed that they could not handle all of these pieces of shit trying to rejoin society at once.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(first_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Word tokenize each chunked sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Noting', 'that', 'the', 'resignation', 'of', 'James', 'Mattis', 'as', 'Secretary', 'of', 'Defense', 'marked', 'the', 'ouster', 'of', 'the', 'third', 'top', 'administration', 'official', 'in', 'less', 'than', 'three', 'weeks', ',', 'a', 'worried', 'populace', 'told', 'reporters', 'Friday', 'that', 'it', 'was', 'unsure', 'how', 'many', 'former', 'Trump', 'staffers', 'it', 'could', 'safely', 'reabsorb', '.'], ['“', 'Jesus', ',', 'we', 'can', '’', 't', 'just', 'take', 'back', 'these', 'assholes', 'all', 'at', 'once—we', 'need', 'time', 'to', 'process', 'one', 'before', 'we', 'get', 'the', 'next', ',', '”', 'said', '53-year-old', 'Gregory', 'Birch', 'of', 'Naperville', ',', 'IL', 'echoing', 'the', 'concerns', 'of', '323', 'million', 'Americans', 'in', 'also', 'noting', 'that', 'the', 'country', 'was', 'only', 'now', 'truly', 'beginning', 'to', 'reintegrate', 'former', 'national', 'security', 'advisor', 'Michael', 'Flynn', '.'], ['“', 'This', 'is', 'just', 'not', 'sustainable', '.'], ['I', '’', 'd', 'say', 'we', 'can', 'handle', 'maybe', 'one', 'or', 'two', 'more', 'former', 'members', 'of', 'Trump', '’', 's', 'inner', 'circle', 'over', 'the', 'remainder', 'of', 'the', 'year', ',', 'but', 'that', '’', 's', 'it', '.'], ['This', 'country', 'has', 'its', 'limits.', '”', 'The', 'U.S.', 'populace', 'confirmed', 'that', 'they', 'could', 'not', 'handle', 'all', 'of', 'these', 'pieces', 'of', 'shit', 'trying', 'to', 'rejoin', 'society', 'at', 'once', '.']]\n"
     ]
    }
   ],
   "source": [
    "print([word_tokenize(sent) for sent in sent_tokenize(first_doc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "List of lists: each sentence, word tokenized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For our use case: \n",
    "- vectorizing documents in word-count vector\n",
    "- word tokenization suffices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Word tokenize each document in collection of documents\n",
    "- List of token lists for each document in collection: **corpus**\n",
    "- Unique tokens in entire corpus: **dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Noting', 'that', 'the', 'resignation', 'of', 'James', 'Mattis', 'as', 'Secretary', 'of', 'Defense', 'marked', 'the', 'ouster', 'of', 'the', 'third', 'top', 'administration', 'official', 'in', 'less', 'than', 'three', 'weeks', ',', 'a', 'worried', 'populace', 'told', 'reporters', 'Friday', 'that', 'it', 'was', 'unsure', 'how', 'many', 'former', 'Trump', 'staffers', 'it', 'could', 'safely', 'reabsorb', '.', '“', 'Jesus', ',', 'we', 'can', '’', 't', 'just', 'take', 'back', 'these', 'assholes', 'all', 'at', 'once—we', 'need', 'time', 'to', 'process', 'one', 'before', 'we', 'get', 'the', 'next', ',', '”', 'said', '53-year-old', 'Gregory', 'Birch', 'of', 'Naperville', ',', 'IL', 'echoing', 'the', 'concerns', 'of', '323', 'million', 'Americans', 'in', 'also', 'noting', 'that', 'the', 'country', 'was', 'only', 'now', 'truly', 'beginning', 'to', 'reintegrate', 'former', 'national', 'security', 'advisor', 'Michael', 'Flynn', '.', '“', 'This', 'is', 'just', 'not', 'sustainable', '.', 'I', '’', 'd', 'say', 'we', 'can', 'handle', 'maybe', 'one', 'or', 'two', 'more', 'former', 'members', 'of', 'Trump', '’', 's', 'inner', 'circle', 'over', 'the', 'remainder', 'of', 'the', 'year', ',', 'but', 'that', '’', 's', 'it', '.', 'This', 'country', 'has', 'its', 'limits.', '”', 'The', 'U.S.', 'populace', 'confirmed', 'that', 'they', 'could', 'not', 'handle', 'all', 'of', 'these', 'pieces', 'of', 'shit', 'trying', 'to', 'rejoin', 'society', 'at', 'once', '.'], ['Desperate', 'to', 'unwind', 'after', 'months', 'of', 'nonstop', 'work', 'investigating', 'Russian', 'influence', 'in', 'the', '2016', 'election', ',', 'visibly', 'exhausted', 'Special', 'Counsel', 'Robert', 'Mueller', 'powered', 'his', 'phone', 'down', 'Friday', 'in', 'order', 'to', 'give', 'himself', 'a', 'break', 'from', 'any', 'news', 'concerning', 'the', 'probe', 'over', 'the', 'holiday', 'break', '.', '“', 'The', 'last', 'thing', 'I', 'want', 'when', 'I', '’', 'm', 'spending', 'time', 'with', 'my', 'family', 'is', 'a', 'cascade', 'of', 'push', 'notifications', 'telling', 'me', 'yet', 'another', 'Russian', 'oligarch', ',', 'political', 'operative', ',', 'or', 'highly', 'placed', 'socialite', 'used', 'Deutsche', 'Bank', 'channels', 'to', 'funnel', 'money', 'to', 'the', 'campaign', ',', '”', 'said', 'the', 'former', 'FBI', 'director', ',', 'firmly', 'holding', 'down', 'his', 'phone', '’', 's', 'power', 'button', 'and', 'adding', 'that', 'he', 'wants', 'to', 'be', '“', 'completely', 'present', 'in', 'the', 'moment', '”', 'while', 'celebrating', 'with', 'his', 'loved', 'ones', ',', 'not', 'ruminating', 'about', 'who', 'met', 'with', 'which', 'diplomat', 'or', 'whether', 'someone', 'was', 'using', 'social', 'media', 'to', 'tamper', 'with', 'his', 'witnesses', '.', '“', 'I', 'just', 'want', 'to', 'have', 'two', 'calm', 'weeks', 'where', 'I', 'don', '’', 't', 'even', 'think', 'about', 'Individual', 'One', '.', 'I', 'won', '’', 't', 'even', 'say', 'his', 'name', '.', 'I', '’', 'll', 'have', 'to', 'wait', 'to', 'hear', 'about', 'any', 'important', 'developments', 'in', 'January', ',', 'since', 'I', 'just', 'know', 'the', 'second', 'I', 'read', ',', 'say', ',', 'something', 'about', 'Eric', 'being', 'involved', 'more', 'deeply', 'than', 'we', 'previously', 'suspected', ',', 'I', '’', 'll', 'get', 'pulled', 'back', 'in', 'and', 'ruin', 'my', 'whole', 'vacation.', '”', 'At', 'press', 'time', ',', 'Mueller', 'had', 'reactivated', 'his', 'phone', 'just', 'to', 'check', 'the', 'news', 'real', 'quick', '.'], ['Nearly', 'halfway', 'through', 'his', 'presidential', 'term', ',', 'Donald', 'Trump', 'has', 'continued', 'to', 'exist', 'in', 'a', 'perpetual', 'state', 'of', 'controversy', ',', 'and', '2018', 'provided', 'no', 'shortage', 'of', 'outrageous', 'moments', '.', 'The', 'Onion', 'looks', 'back', 'at', 'the', 'most', 'significant', 'events', 'in', 'the', 'Trump', 'presidency', 'in', '2018', '.'], ['Attempting', 'to', 'make', 'amends', 'for', 'gross', 'abuses', 'of', 'power', 'during', 'his', 'time', 'as', 'Interior', 'Department', 'Secretary', ',', 'an', 'unusually', 'contrite', 'Ryan', 'Zinke', 'apologized', 'Monday', 'for', 'misusing', 'government', 'funds', 'by', 'sending', 'the', 'members', 'of', 'the', 'ethics', 'committee', 'a', '$', '160,000', 'vase', '.', '“', 'I', 'know', 'this', 'doesn', '’', 't', 'change', 'anything', 'about', 'how', 'I', 'exploited', 'my', 'cabinet', 'position', ',', 'but', 'I', 'hope', 'you', 'will', 'accept', 'this', 'beautiful', 'example', 'of', 'Qing', 'dynasty', 'porcelain', 'as', 'a', 'small', 'token', 'of', 'my', 'regret', ',', '”', 'said', 'Zinke', ',', 'acknowledging', 'that', 'while', 'no', 'gift', 'could', 'make', 'up', 'for', 'the', 'time', 'he', 'spent', '$', '139,000', 'of', 'taxpayer', 'money', 'to', 'renovate', 'his', 'office', 'doors', ',', 'he', 'hoped', 'the', 'committee', 'would', 'consider', 'the', 'vase', 'as', 'a', 'sincere', 'gesture', 'of', 'apology', '.', '“', 'I', 'was', 'wrong', 'to', 'take', 'advantage', 'of', 'my', 'position', ',', 'and', 'I', 'hope', 'the', 'lustrous', 'glazing', 'and', 'firing', 'evident', 'in', 'this', 'piece', 'will', 'move', 'you', 'to', 'forgive', 'me', 'for', 'my', 'all-too-human', 'failings', '.', 'Please', 'don', '’', 't', 'remember', 'me', 'as', 'the', 'man', 'who', 'used', 'government', 'funds', 'on', 'security', 'detail', 'for', 'my', 'family', 'while', 'we', 'were', 'on', 'vacation', 'in', 'Turkey', ',', 'or', 'as', 'the', 'man', 'who', 'violated', 'the', 'Hatch', 'Act', 'and', 'acted', 'as', 'a', 'pawn', 'for', 'the', 'oil', 'and', 'gas', 'industry', '.', 'Rather', ',', 'when', 'your', 'eyes', 'happen', 'to', 'fall', 'on', 'this', 'unique', 'example', 'of', 'kaolin', 'clay-work', ',', 'I', 'hope', 'you', 'will', 'remember', 'me', 'as', 'the', 'man', 'who', 'bought', 'each', 'of', 'you', 'the', '2019', 'Mercedes', 'Benz', 'S560', 'sedans', 'you', '’', 'll', 'find', 'in', 'the', 'parking', 'lot', 'when', 'you', 'leave', 'today.', '”', 'Zinke', 'further', 'plans', 'to', 'apologize', 'in', 'person', 'to', 'each', 'committee', 'member', 'by', 'visiting', 'them', 'at', 'their', 'homes', 'using', 'a', 'government', 'helicopter', '.']]\n"
     ]
    }
   ],
   "source": [
    "corpus = [word_tokenize(doc) for doc in satire_df['body']]\n",
    "print(corpus[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For purposes of understanding the dictionary/vocabulary:\n",
    "- flattening corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(464861,)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "flattenedcorpus_tokens = pd.Series(list(itertools.chain(*corpus)))\n",
    "print(flattenedcorpus_tokens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dictionary, then, is unique values of tokens in corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30182\n"
     ]
    }
   ],
   "source": [
    "dictionary = pd.Series(\n",
    "    flattenedcorpus_tokens.unique())\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ",               21510\n",
       "the             21378\n",
       ".               16432\n",
       "to              11244\n",
       "of              10582\n",
       "                ...  \n",
       "insert              1\n",
       "kidney              1\n",
       "ovaries             1\n",
       "inhabit             1\n",
       "inter-island        1\n",
       "Length: 30182, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattenedcorpus_tokens.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Tokens in the dictionary become features for a token-frequency matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"Images/vectorchart.png\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this light, think about the dictionary:\n",
    "\n",
    "- any problems?\n",
    "- look at various types of tokens. anything that you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 30,000 features: way too much. Curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Problem 2\n",
    "- Want features to help us in classification task\n",
    "- But many useless features: tokens too common in english language.\n",
    "    - punctuation\n",
    "    - prepositions, articles, etc.: **stop words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ",       21510\n",
       "the     21378\n",
       ".       16432\n",
       "to      11244\n",
       "of      10582\n",
       "and      9997\n",
       "a        9361\n",
       "in       8066\n",
       "’        4828\n",
       "is       4762\n",
       "that     4153\n",
       "on       3904\n",
       "for      3884\n",
       "s        3354\n",
       "“        3034\n",
       "”        2889\n",
       "The      2818\n",
       "said     2661\n",
       "with     2559\n",
       "as       2431\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattenedcorpus_tokens.value_counts()[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattenedcorpus_tokens.isin([\"warning\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattenedcorpus_tokens.isin([\"Warning\"]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Same exact word: just capitalized\n",
    "- Shouldn't be independent feature.\n",
    "- lowercase all of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattenedcorpus_tokens.isin([\"warns\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattenedcorpus_tokens.isin([\"warned\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattenedcorpus_tokens.isin([\"warn\"]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "All of these are treated as unique features:\n",
    "- but are just variant of same word\n",
    "- need to normalize these in some way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's get the number of tokens with only one occurence in entire corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22301"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_one_occurence = (flattenedcorpus_tokens.\n",
    "                     value_counts() < 5).sum()\n",
    "num_one_occurence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "~ 1/3 of tokens only appear **once**!\n",
    "\n",
    "- Rare token are not useful to keep around.\n",
    "- Not useful in building relationship between features and target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Many of these tokens are numbers: \n",
    "- don't have semantic meaning that will aid in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71        323\n",
       "129      2016\n",
       "260      2018\n",
       "369      2019\n",
       "505      2020\n",
       "         ... \n",
       "29810    1209\n",
       "29820     177\n",
       "29893      77\n",
       "29957     152\n",
       "29991     167\n",
       "Length: 398, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[dictionary.str.isnumeric()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Addressing these problems step-by-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Lower casing, removing punctuation, and stop words.\n",
    "- Keep only alphabetic tokens (drop numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we']\n"
     ]
    }
   ],
   "source": [
    "# imports package with many stopword lists\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# get common stop words in english that we'll remove during tokenization/text normalization\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Create a simple helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def first_step_normalizer(doc):\n",
    "    # filters for alphabetic (no punctuation or numbers) and filters out stop words. \n",
    "    # lower cases all tokens\n",
    "    norm_text = [x.lower() for x in word_tokenize(doc) if ((x.isalpha()) & (x not in stop_words)) ]\n",
    "    return norm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>target</th>\n",
       "      <th>tok_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Noting that the resignation of James Mattis as...</td>\n",
       "      <td>1</td>\n",
       "      <td>[noting, resignation, james, mattis, secretary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Desperate to unwind after months of nonstop wo...</td>\n",
       "      <td>1</td>\n",
       "      <td>[desperate, unwind, months, nonstop, work, inv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nearly halfway through his presidential term, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[nearly, halfway, presidential, term, donald, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Attempting to make amends for gross abuses of ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[attempting, make, amends, gross, abuses, powe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decrying the Senate’s resolution blaming the c...</td>\n",
       "      <td>1</td>\n",
       "      <td>[decrying, senate, resolution, blaming, crown,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  target  \\\n",
       "0  Noting that the resignation of James Mattis as...       1   \n",
       "1  Desperate to unwind after months of nonstop wo...       1   \n",
       "2  Nearly halfway through his presidential term, ...       1   \n",
       "3  Attempting to make amends for gross abuses of ...       1   \n",
       "4  Decrying the Senate’s resolution blaming the c...       1   \n",
       "\n",
       "                                            tok_norm  \n",
       "0  [noting, resignation, james, mattis, secretary...  \n",
       "1  [desperate, unwind, months, nonstop, work, inv...  \n",
       "2  [nearly, halfway, presidential, term, donald, ...  \n",
       "3  [attempting, make, amends, gross, abuses, powe...  \n",
       "4  [decrying, senate, resolution, blaming, crown,...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "satire_df['tok_norm'] = satire_df['body'].apply(first_step_normalizer)\n",
    "satire_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['noting', 'resignation', 'james', 'mattis', 'secretary', 'defense', 'marked', 'ouster', 'third', 'top', 'administration', 'official', 'less', 'three', 'weeks', 'worried', 'populace', 'told', 'reporters', 'friday', 'unsure', 'many', 'former', 'trump', 'staffers', 'could', 'safely', 'reabsorb', 'jesus', 'take', 'back', 'assholes', 'need', 'time', 'process', 'one', 'get', 'next', 'said', 'gregory', 'birch', 'naperville', 'il', 'echoing', 'concerns', 'million', 'americans', 'also', 'noting', 'country', 'truly', 'beginning', 'reintegrate', 'former', 'national', 'security', 'advisor', 'michael', 'flynn', 'this', 'sustainable', 'i', 'say', 'handle', 'maybe', 'one', 'two', 'former', 'members', 'trump', 'inner', 'circle', 'remainder', 'year', 'this', 'country', 'the', 'populace', 'confirmed', 'could', 'handle', 'pieces', 'shit', 'trying', 'rejoin', 'society']\n"
     ]
    }
   ],
   "source": [
    "print(satire_df['tok_norm'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23067\n"
     ]
    }
   ],
   "source": [
    "norm_toks_flattened = pd.Series(list(\n",
    "    itertools.chain(*satire_df['tok_norm'])))\n",
    "new_dictionary = norm_toks_flattened.unique()\n",
    "print(len(new_dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Process removed 7000 features from the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30182\n"
     ]
    }
   ],
   "source": [
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Next step: stemming/lemmatizing\n",
    "- Converting variants of the same word to a base form or root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Stemmers consolidate similar words by chopping off the ends of the words.\n",
    "<center><img src = \"Images/stemmer.png\" width = 200> Stem isn't always a word.</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Different stemming algorithms (in order of increasing aggression):\n",
    "- Porter stemmer\n",
    "- Snowball stemmer (faster, more aggressive, smarter)\n",
    "- Lancaster stemmer (real aggresso, **ultrafast**)\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"Images/stemmers.jpg\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()\n",
    "s_stemmer = SnowballStemmer(language=\"english\")\n",
    "l_stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Running a Porter stemmer on a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "cell_style": "split",
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['noting', 'resignation', 'james', 'mattis', 'secretary', 'defense', 'marked', 'ouster', 'third', 'top', 'administration', 'official', 'less', 'three', 'weeks', 'worried', 'populace', 'told', 'reporters', 'friday', 'unsure', 'many', 'former', 'trump', 'staffers', 'could', 'safely', 'reabsorb', 'jesus', 'take', 'back', 'assholes', 'need', 'time', 'process', 'one', 'get', 'next', 'said', 'gregory', 'birch', 'naperville', 'il', 'echoing', 'concerns', 'million', 'americans', 'also', 'noting', 'country', 'truly', 'beginning', 'reintegrate', 'former', 'national', 'security', 'advisor', 'michael', 'flynn', 'this', 'sustainable', 'i', 'say', 'handle', 'maybe', 'one', 'two', 'former', 'members', 'trump', 'inner', 'circle', 'remainder', 'year', 'this', 'country', 'the', 'populace', 'confirmed', 'could', 'handle', 'pieces', 'shit', 'trying', 'rejoin', 'society']\n"
     ]
    }
   ],
   "source": [
    "sample_doc = satire_df['tok_norm'].iloc[0]\n",
    "print(sample_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    ".stem(token) method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['note', 'resign', 'jame', 'matti', 'secretari', 'defens', 'mark', 'ouster', 'third', 'top', 'administr', 'offici', 'less', 'three', 'week', 'worri', 'populac', 'told', 'report', 'friday', 'unsur', 'mani', 'former', 'trump', 'staffer', 'could', 'safe', 'reabsorb', 'jesu', 'take', 'back', 'asshol', 'need', 'time', 'process', 'one', 'get', 'next', 'said', 'gregori', 'birch', 'napervil', 'il', 'echo', 'concern', 'million', 'american', 'also', 'note', 'countri', 'truli', 'begin', 'reintegr', 'former', 'nation', 'secur', 'advisor', 'michael', 'flynn', 'thi', 'sustain', 'i', 'say', 'handl', 'mayb', 'one', 'two', 'former', 'member', 'trump', 'inner', 'circl', 'remaind', 'year', 'thi', 'countri', 'the', 'populac', 'confirm', 'could', 'handl', 'piec', 'shit', 'tri', 'rejoin', 'societi']\n"
     ]
    }
   ],
   "source": [
    "port_stemmed_doc  = [p_stemmer.stem(token) \n",
    "                     for token in sample_doc]\n",
    "print(port_stemmed_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Compare Porter and Snowball stemmer on a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['note', 'resign', 'jame', 'matti', 'secretari', 'defens', 'mark', 'ouster', 'third', 'top', 'administr', 'offici', 'less', 'three', 'week', 'worri', 'populac', 'told', 'report', 'friday', 'unsur', 'mani', 'former', 'trump', 'staffer', 'could', 'safe', 'reabsorb', 'jesu', 'take', 'back', 'asshol', 'need', 'time', 'process', 'one', 'get', 'next', 'said', 'gregori', 'birch', 'napervil', 'il', 'echo', 'concern', 'million', 'american', 'also', 'note', 'countri', 'truli', 'begin', 'reintegr', 'former', 'nation', 'secur', 'advisor', 'michael', 'flynn', 'thi', 'sustain', 'i', 'say', 'handl', 'mayb', 'one', 'two', 'former', 'member', 'trump', 'inner', 'circl', 'remaind', 'year', 'thi', 'countri', 'the', 'populac', 'confirm', 'could', 'handl', 'piec', 'shit', 'tri', 'rejoin', 'societi']\n"
     ]
    }
   ],
   "source": [
    "print(port_stemmed_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['note', 'resign', 'jame', 'matti', 'secretari', 'defens', 'mark', 'ouster', 'third', 'top', 'administr', 'offici', 'less', 'three', 'week', 'worri', 'populac', 'told', 'report', 'friday', 'unsur', 'mani', 'former', 'trump', 'staffer', 'could', 'safe', 'reabsorb', 'jesus', 'take', 'back', 'asshol', 'need', 'time', 'process', 'one', 'get', 'next', 'said', 'gregori', 'birch', 'napervill', 'il', 'echo', 'concern', 'million', 'american', 'also', 'note', 'countri', 'truli', 'begin', 'reintegr', 'former', 'nation', 'secur', 'advisor', 'michael', 'flynn', 'this', 'sustain', 'i', 'say', 'handl', 'mayb', 'one', 'two', 'former', 'member', 'trump', 'inner', 'circl', 'remaind', 'year', 'this', 'countri', 'the', 'populac', 'confirm', 'could', 'handl', 'piec', 'shit', 'tri', 'rejoin', 'societi']\n"
     ]
    }
   ],
   "source": [
    "snowball_stemmed_doc  = [s_stemmer.stem(token) \n",
    "                     for token in sample_doc]\n",
    "print(snowball_stemmed_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nearly identical results. Snowball is generally faster. Often also better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Marked difference in results between Porter/Snowball vs. Lancaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['note', 'resign', 'jame', 'matti', 'secretari', 'defens', 'mark', 'ouster', 'third', 'top', 'administr', 'offici', 'less', 'three', 'week', 'worri', 'populac', 'told', 'report', 'friday', 'unsur', 'mani', 'former', 'trump', 'staffer', 'could', 'safe', 'reabsorb', 'jesus', 'take', 'back', 'asshol', 'need', 'time', 'process', 'one', 'get', 'next', 'said', 'gregori', 'birch', 'napervill', 'il', 'echo', 'concern', 'million', 'american', 'also', 'note', 'countri', 'truli', 'begin', 'reintegr', 'former', 'nation', 'secur', 'advisor', 'michael', 'flynn', 'this', 'sustain', 'i', 'say', 'handl', 'mayb', 'one', 'two', 'former', 'member', 'trump', 'inner', 'circl', 'remaind', 'year', 'this', 'countri', 'the', 'populac', 'confirm', 'could', 'handl', 'piec', 'shit', 'tri', 'rejoin', 'societi']\n"
     ]
    }
   ],
   "source": [
    "print(snowball_stemmed_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'resign', 'jam', 'mat', 'secret', 'defens', 'mark', 'oust', 'third', 'top', 'admin', 'off', 'less', 'three', 'week', 'worry', 'populac', 'told', 'report', 'friday', 'uns', 'many', 'form', 'trump', 'staff', 'could', 'saf', 'reabsorb', 'jes', 'tak', 'back', 'asshol', 'nee', 'tim', 'process', 'on', 'get', 'next', 'said', 'greg', 'birch', 'napervil', 'il', 'echo', 'concern', 'mil', 'am', 'also', 'not', 'country', 'tru', 'begin', 'reintegr', 'form', 'nat', 'sec', 'adv', 'michael', 'flyn', 'thi', 'sustain', 'i', 'say', 'handl', 'mayb', 'on', 'two', 'form', 'memb', 'trump', 'in', 'circ', 'remaind', 'year', 'thi', 'country', 'the', 'populac', 'confirm', 'could', 'handl', 'piec', 'shit', 'try', 'rejoin', 'socy']\n"
     ]
    }
   ],
   "source": [
    "lancaster_stemmed_doc  = [l_stemmer.stem(token) \n",
    "                     for token in sample_doc]\n",
    "\n",
    "print(lancaster_stemmed_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Advantages/Disadvantages of stemming:\n",
    "- Uses simple, **fast** tree-based algorithms to normalize word variants\n",
    "- Stems not always words\n",
    "- Can produce base forms that are pretty weird/merge different words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Lemmatization\n",
    "\n",
    "- Another way to convert inflections of word to a base form \n",
    "- Not simply cutting to word root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Changes to word *lemma*:\n",
    "- is, was, will $\\rightarrow$ be\n",
    "- haves, having, had $\\rightarrow$ have\n",
    "- leafs, leaves $\\rightarrow$ leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This enhanced ability comes at a small cost:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Requires part of speech (POS) information\n",
    "- due to possible ambiguities in form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Example:\n",
    "- *leaves* (verb or noun)\n",
    "- *leaves* (noun) $\\rightarrow$ leaf\n",
    "- *leaves* (verb) $\\rightarrow$ leave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "nltk has implementation of the WordNet Lemmatizer:\n",
    "- links into Wordnet\n",
    "- the mother of all semantic/lexical databases\n",
    "- stores library of contextual word relationships, POS tagging, etc.\n",
    "- *excellent* for rule-based document parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/wordnet.webp\" >\n",
    "<center><a href = \"https://wordnet.princeton.edu/\" >Princeton's WordNet</a> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer # lemmatizer using WordNet\n",
    "from nltk.corpus import wordnet # imports WordNet\n",
    "from nltk import pos_tag # nltk's native part of speech tagging\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Part of Speech (POS) Tagging\n",
    "\n",
    "- identify parts of speech of each token from ordered list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sent_string = \"The dog licked the babies in the face.\"\n",
    "sent_tok_list = word_tokenize(sent_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'dog', 'licked', 'the', 'babies', 'in', 'the', 'face', '.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tok_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('dog', 'NN'),\n",
       " ('licked', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('babies', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('face', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(sent_tok_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a href = \"https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\">List of NLTK POS tags</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use POS tagging in lemmatizer, but:\n",
    "- WordNet has different POS tagging system.\n",
    "- Helper function to convert (reuse this code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# helper function to change nltk's part of speech tagging to a wordnet format.\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see this tagging in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('noting', 'v'), ('resignation', 'n'), ('james', 'n'), ('mattis', 'v'), ('secretary', 'n'), ('defense', 'n'), ('marked', 'v'), ('ouster', 'a'), ('third', 'a'), ('top', 'a'), ('administration', 'n'), ('official', 'n'), ('less', 'a'), ('three', None), ('weeks', 'n'), ('worried', 'v'), ('populace', 'n'), ('told', 'v'), ('reporters', 'n'), ('friday', 'a'), ('unsure', 'a'), ('many', 'a'), ('former', 'a'), ('trump', 'n'), ('staffers', 'n'), ('could', None), ('safely', 'r'), ('reabsorb', 'v'), ('jesus', 'n'), ('take', 'v'), ('back', 'r'), ('assholes', 'n'), ('need', 'v'), ('time', 'n'), ('process', 'n'), ('one', None), ('get', 'n'), ('next', 'a'), ('said', 'v'), ('gregory', 'a'), ('birch', 'n'), ('naperville', 'n'), ('il', 'n'), ('echoing', 'v'), ('concerns', 'n'), ('million', None), ('americans', 'n'), ('also', 'r'), ('noting', 'v'), ('country', 'n'), ('truly', 'r'), ('beginning', 'v'), ('reintegrate', 'v'), ('former', 'a'), ('national', 'a'), ('security', 'n'), ('advisor', 'n'), ('michael', 'n'), ('flynn', 'v'), ('this', None), ('sustainable', 'a'), ('i', 'n'), ('say', 'v'), ('handle', 'a'), ('maybe', 'r'), ('one', None), ('two', None), ('former', 'a'), ('members', 'n'), ('trump', 'v'), ('inner', 'a'), ('circle', 'n'), ('remainder', 'n'), ('year', 'n'), ('this', None), ('country', 'n'), ('the', None), ('populace', 'n'), ('confirmed', 'v'), ('could', None), ('handle', 'v'), ('pieces', 'n'), ('shit', 'v'), ('trying', 'v'), ('rejoin', 'a'), ('society', 'n')]\n"
     ]
    }
   ],
   "source": [
    "# document to list of tuples with tokens and POS tags in nltk format\n",
    "# converts to wordnet format\n",
    "\n",
    "wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tag(sample_doc))) \n",
    "print(wordnet_tagged)\n",
    "# [wnl.lemmatize(x[0], x[1]) for x in wordnet_tagged if x[1] is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This format can be inputted directly into WordNet lemmatizer.\n",
    "\n",
    "- Instantiate wordnet object:\n",
    "- WordNetLemmatizer()\n",
    "- has method .lemmatize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['note', 'resignation', 'james', 'mattis', 'secretary', 'defense', 'mark', 'ouster', 'third', 'top', 'administration', 'official', 'less', 'week', 'worry', 'populace', 'tell', 'reporter', 'friday', 'unsure', 'many', 'former', 'trump', 'staffer', 'safely', 'reabsorb', 'jesus', 'take', 'back', 'asshole', 'need', 'time', 'process', 'get', 'next', 'say', 'gregory', 'birch', 'naperville', 'il', 'echo', 'concern', 'american', 'also', 'note', 'country', 'truly', 'begin', 'reintegrate', 'former', 'national', 'security', 'advisor', 'michael', 'flynn', 'sustainable', 'i', 'say', 'handle', 'maybe', 'former', 'member', 'trump', 'inner', 'circle', 'remainder', 'year', 'country', 'populace', 'confirm', 'handle', 'piece', 'shit', 'try', 'rejoin', 'society']\n"
     ]
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "doc_lemmatized = [wnl.lemmatize(token, pos) for token, pos in wordnet_tagged if pos is not None]\n",
    "print(doc_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Compare original tokens and lemmatized tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['noting', 'resignation', 'james', 'mattis', 'secretary', 'defense', 'marked', 'ouster', 'third', 'top', 'administration', 'official', 'less', 'three', 'weeks', 'worried', 'populace', 'told', 'reporters', 'friday', 'unsure', 'many', 'former', 'trump', 'staffers', 'could', 'safely', 'reabsorb', 'jesus', 'take', 'back', 'assholes', 'need', 'time', 'process', 'one', 'get', 'next', 'said', 'gregory', 'birch', 'naperville', 'il', 'echoing', 'concerns', 'million', 'americans', 'also', 'noting', 'country', 'truly', 'beginning', 'reintegrate', 'former', 'national', 'security', 'advisor', 'michael', 'flynn', 'this', 'sustainable', 'i', 'say', 'handle', 'maybe', 'one', 'two', 'former', 'members', 'trump', 'inner', 'circle', 'remainder', 'year', 'this', 'country', 'the', 'populace', 'confirmed', 'could', 'handle', 'pieces', 'shit', 'trying', 'rejoin', 'society']\n"
     ]
    }
   ],
   "source": [
    "print(sample_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['note', 'resignation', 'james', 'mattis', 'secretary', 'defense', 'mark', 'ouster', 'third', 'top', 'administration', 'official', 'less', 'week', 'worry', 'populace', 'tell', 'reporter', 'friday', 'unsure', 'many', 'former', 'trump', 'staffer', 'safely', 'reabsorb', 'jesus', 'take', 'back', 'asshole', 'need', 'time', 'process', 'get', 'next', 'say', 'gregory', 'birch', 'naperville', 'il', 'echo', 'concern', 'american', 'also', 'note', 'country', 'truly', 'begin', 'reintegrate', 'former', 'national', 'security', 'advisor', 'michael', 'flynn', 'sustainable', 'i', 'say', 'handle', 'maybe', 'former', 'member', 'trump', 'inner', 'circle', 'remainder', 'year', 'country', 'populace', 'confirm', 'handle', 'piece', 'shit', 'try', 'rejoin', 'society']\n"
     ]
    }
   ],
   "source": [
    "print(doc_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Compare snowball stemmer and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['note', 'resign', 'jame', 'matti', 'secretari', 'defens', 'mark', 'ouster', 'third', 'top', 'administr', 'offici', 'less', 'three', 'week', 'worri', 'populac', 'told', 'report', 'friday', 'unsur', 'mani', 'former', 'trump', 'staffer', 'could', 'safe', 'reabsorb', 'jesus', 'take', 'back', 'asshol', 'need', 'time', 'process', 'one', 'get', 'next', 'said', 'gregori', 'birch', 'napervill', 'il', 'echo', 'concern', 'million', 'american', 'also', 'note', 'countri', 'truli', 'begin', 'reintegr', 'former', 'nation', 'secur', 'advisor', 'michael', 'flynn', 'this', 'sustain', 'i', 'say', 'handl', 'mayb', 'one', 'two', 'former', 'member', 'trump', 'inner', 'circl', 'remaind', 'year', 'this', 'countri', 'the', 'populac', 'confirm', 'could', 'handl', 'piec', 'shit', 'tri', 'rejoin', 'societi']\n"
     ]
    }
   ],
   "source": [
    "print(snowball_stemmed_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['note', 'resignation', 'james', 'mattis', 'secretary', 'defense', 'mark', 'ouster', 'third', 'top', 'administration', 'official', 'less', 'week', 'worry', 'populace', 'tell', 'reporter', 'friday', 'unsure', 'many', 'former', 'trump', 'staffer', 'safely', 'reabsorb', 'jesus', 'take', 'back', 'asshole', 'need', 'time', 'process', 'get', 'next', 'say', 'gregory', 'birch', 'naperville', 'il', 'echo', 'concern', 'american', 'also', 'note', 'country', 'truly', 'begin', 'reintegrate', 'former', 'national', 'security', 'advisor', 'michael', 'flynn', 'sustainable', 'i', 'say', 'handle', 'maybe', 'former', 'member', 'trump', 'inner', 'circle', 'remainder', 'year', 'country', 'populace', 'confirm', 'handle', 'piece', 'shit', 'try', 'rejoin', 'society']\n"
     ]
    }
   ],
   "source": [
    "print(doc_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lemmatization: \n",
    "- far superior to stemming in terms of semantic text normalization\n",
    "- but need good POS tagging.\n",
    "- slower than stemming: issue for processing large amounts of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Applying lemmatizer to corpus\n",
    "- useful to all preprocessing steps/necessary subroutines into one function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# takes in untokenized document and returns fully normalized token list\n",
    "def process_doc(doc):\n",
    "\n",
    "    #initialize lemmatizer\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    # helper function to change nltk's part of speech tagging to a wordnet format.\n",
    "    def pos_tagger(nltk_tag):\n",
    "        if nltk_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif nltk_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif nltk_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif nltk_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:         \n",
    "            return None\n",
    "        \n",
    "    # remove stop words and punctuations, then lower case\n",
    "    doc_norm = [tok.lower() for tok in word_tokenize(doc) if ((tok.isalpha()) & (tok not in stop_words)) ]\n",
    "\n",
    "    #  POS detection on the result will be important in telling Wordnet's lemmatizer how to lemmatize\n",
    "    \n",
    "    # creates list of tuples with tokens and POS tags in wordnet format\n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tag(doc_norm))) \n",
    "    doc_norm = [wnl.lemmatize(token, pos) for token, pos in wordnet_tagged if pos is not None]\n",
    "    \n",
    "    return doc_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['note', 'resignation', 'james', 'mattis', 'secretary', 'defense', 'mark', 'ouster', 'third', 'top', 'administration', 'official', 'less', 'week', 'worry', 'populace', 'tell', 'reporter', 'friday', 'unsure', 'many', 'former', 'trump', 'staffer', 'safely', 'reabsorb', 'jesus', 'take', 'back', 'asshole', 'need', 'time', 'process', 'get', 'next', 'say', 'gregory', 'birch', 'naperville', 'il', 'echo', 'concern', 'american', 'also', 'note', 'country', 'truly', 'begin', 'reintegrate', 'former', 'national', 'security', 'advisor', 'michael', 'flynn', 'sustainable', 'i', 'say', 'handle', 'maybe', 'former', 'member', 'trump', 'inner', 'circle', 'remainder', 'year', 'country', 'populace', 'confirm', 'handle', 'piece', 'shit', 'try', 'rejoin', 'society']\n"
     ]
    }
   ],
   "source": [
    "print(process_doc(satire_df['body'].iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Apply text tokenization/normalization to whole body of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fully_normalized_corpus = satire_df['body'].apply(process_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [note, resignation, james, mattis, secretary, ...\n",
       "1    [desperate, unwind, month, nonstop, work, inve...\n",
       "2    [nearly, halfway, presidential, term, donald, ...\n",
       "3    [attempt, make, amends, gross, abuse, power, t...\n",
       "4    [decry, senate, resolution, blame, crown, prin...\n",
       "Name: body, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fully_normalized_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18484"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_fully_norm = pd.Series(list(itertools.chain(*fully_normalized_corpus)))\n",
    "len(flattened_fully_norm.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Original dictionary length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30182\n"
     ]
    }
   ],
   "source": [
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Removed/cleaned dictionary to around half its size:\n",
    "- Normalized text appropriately\n",
    "- Still not dealt with infrequent tokens\n",
    "- Tokens too common but not in stop words list.\n",
    "- Will do when vectorizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's flatten the lists and save to csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "cell_style": "split",
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fnc_output = fully_normalized_corpus.apply(\n",
    "    \" \".join)\n",
    "\n",
    "fnc_output.to_csv(\"data/satire_norm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      note resignation james mattis secretary defens...\n",
       "1      desperate unwind month nonstop work investigat...\n",
       "2      nearly halfway presidential term donald trump ...\n",
       "3      attempt make amends gross abuse power time int...\n",
       "4      decry senate resolution blame crown prince bru...\n",
       "                             ...                        \n",
       "995    britain opposition leader jeremy corbyn push a...\n",
       "996    turkey take fight islamic state militant syria...\n",
       "997    malaysia seek reparation goldman sachs group i...\n",
       "998    israeli court sentence palestinian year impris...\n",
       "999    least people die due landslide flood trigger t...\n",
       "Name: body, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnc_output"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
