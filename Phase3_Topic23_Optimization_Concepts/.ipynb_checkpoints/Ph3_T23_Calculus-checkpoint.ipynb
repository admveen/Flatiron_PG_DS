{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:200%;\n",
    "           font-\\amily:Arial;letter-spacing:0.5px\">\n",
    "\n",
    "<p width = 20%, style=\"padding: 10px;\n",
    "              color:white;\">\n",
    "Calculus in 1 hour\n",
    "              \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Data Science Cohort Live NYC Feb 2022\n",
    "<p>Phase 3: Topic 23</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align = \"right\">\n",
    "<img src=\"Images/flatiron-school-logo.png\" align = \"right\" width=\"200\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Calculus at Harvard](https://online-learning.harvard.edu/sites/default/files/styles/header/public/course/asset-v1_HarvardX%2BCalcAPL1x%2B2T2017%2Btype%40asset%2Bblock%40TITLE-Calculus-Applied-2120x1192-NO-SPOTLIGHT%202.png?itok=crWwjmVi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center> All these formulas will be on the code challenge. </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c anaconda sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: `sympy` is not a package that comes included in the `learn-env`. You can install it by running the command above here in the notebook or in your terminal (with `learn-env` activated). You might have to restart the kernel after installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The derivative:\n",
    "\n",
    "<img src = \"Images/derivatives.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Derivative: $$ \\frac{df(x)}{dx} $$\n",
    "\n",
    "The instaneous slope of $f(x)$ at $x = c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Define as limit of slope of chord between f(x + h) and f(x): $$\\frac{\\Delta y}{\\Delta x} $$ \n",
    "- As $\\Delta x \\rightarrow 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The concept of \"instaneous slope\" very useful for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Often calculating the minimum of a cost function:\n",
    "\n",
    "$$L = \\sum_{i=1}^N (y_i - x_i w_1)^2 $$\n",
    "(e.g. for simple linear regression: without bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is a parabola."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " Where there is minimum (maximum): derivative goes to zero:\n",
    " \n",
    " <center><img src = \"Images/parabola_min.gif\" width = 500/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Point Q is where: $$ \\frac{dy}{dx} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finding $w_1$ minimizing cost function:\n",
    "\n",
    "$$L = \\sum_{i=1}^N (y_i - x_i w_1)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Same as finding:\n",
    "\n",
    "$w_1^*$  where derivative vanishes:\n",
    "\n",
    "$$ \\frac{dL}{dw_1} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Problem easy enough (if you know calc) you can solve $w_1^*$ by hand.\n",
    "\n",
    "- Can also use sympy to get us the derivative symbolically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "cell_style": "center",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\sum_{i=1}^{N} \\left(- w_{1} {x}_{i} + {y}_{i}\\right)^{2}$"
      ],
      "text/plain": [
       "Sum((-w1*x[i] + y[i])**2, (i, 1, N))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import *\n",
    "from sympy.abc import x, y\n",
    "\n",
    "x, y, i, N, w1 = symbols(\"x, y, i, N, w1\")\n",
    "L = summation((Indexed('y',i) - Indexed('x',i)*w1)**2 ,(i,1,N))\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is our loss function defined in sympy's symbolic representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Get derivative with sympy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "cell_style": "split",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\sum_{i=1}^{N} \\left(- w_{1} {x}_{i} + {y}_{i}\\right)^{2}$"
      ],
      "text/plain": [
       "Sum((-w1*x[i] + y[i])**2, (i, 1, N))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\sum_{i=1}^{N} - 2 \\left(- w_{1} {x}_{i} + {y}_{i}\\right) {x}_{i}$"
      ],
      "text/plain": [
       "Sum(-2*(-w1*x[i] + y[i])*x[i], (i, 1, N))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = diff(L, w1)\n",
    "grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Set this derivative to zero. Solve for $w_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ w_1 = \\frac{\\sum_i y_ix_i}{\\sum_i x_i^2} $$\n",
    "\n",
    "Should look pretty familiar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$w_1$ is value which minimizes cost function. \n",
    "- Closed form solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Derivatives also aids in:\n",
    "- locally approximating changes in function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Around $x_0$:\n",
    "$$ \\Delta f = \\frac{df}{dx}\\Big|_{x_0}\\Delta x $$\n",
    "<img src = \"Images/linear_approx.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Derivatives in higher dimensions: the partial derivative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Derivative with repect to one variable holding others **fixed**.\n",
    "\n",
    "<img src=\"Images/partial_derivative_as_slope.png\" width = 400/>\n",
    "\n",
    "- $ \\frac{\\partial f}{\\partial x_i} $ partial derivative of $f$ with respect to $x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x, y = symbols('x, y')\n",
    "# function of x and y (a parabaloid)\n",
    "f = x**2 - 4*x*y + y**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2 x - 4 y$"
      ],
      "text/plain": [
       "2*x - 4*y"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#partial derivative with respect to x\n",
    "diff(f,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 4 x + 2 y$"
      ],
      "text/plain": [
       "-4*x + 2*y"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# partial derivative with respect to y\n",
    "diff(f,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The partial derivatives: locally approximating function $f(x,y)$.\n",
    "\n",
    "$$ \\Delta f \\approx \\frac{\\partial f}{\\partial x}\\Delta x + \\frac{\\partial f}{\\partial y}\\Delta y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = \"Images\\plane_approx.jpg\" width = 500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's use dot product and vector magic to rewrite:\n",
    "\n",
    "$$ \\Delta f \\approx \\frac{\\partial f}{\\partial x}\\Delta x + \\frac{\\partial f}{\\partial y}\\Delta y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Define two vectors: \n",
    "$$ \\Delta\\textbf{r} = \\begin{bmatrix} \\Delta x \\\\ \\Delta  y \\end{bmatrix}$$\n",
    "\n",
    "Our actual displacement vector from a point $(x_0, y_0)$.\n",
    "\n",
    "I.e: Start at $(x_0, y_0)$. Go $\\Delta x$ in x direction. Go $\\Delta y$  in y direction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Define a new vector:\n",
    "$$ \\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x}\\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix} $$\n",
    "\n",
    "This is known as the **gradient**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A little dot product action:\n",
    "\n",
    "$$ \\Delta f = \\nabla f \\cdot \\Delta\\textbf{r} = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y} \\end{bmatrix} \\begin{bmatrix} \\Delta x \\\\ \\Delta  y \\end{bmatrix} = \\frac{\\partial f}{\\partial x}\\Delta x + \\frac{\\partial f}{\\partial y}\\Delta y $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ok...I rewrote an equation. Who cares?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Gradient** as vector is useful and important idea.\n",
    "-Let's see why:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "A unit vector $\\textbf{u}$: \n",
    "- Vector of length 1.\n",
    "- $|\\textbf{u}| = 1$\n",
    "- $\\textbf{u} \\cdot \\textbf{u} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "Two unit vectors: \n",
    "- $\\textbf{u}_x$ pointing along x\n",
    "$$\\textbf{u}_x = \\begin{bmatrix} 1 \\\\ \\ 0 \\end{bmatrix} $$\n",
    "- $\\textbf{u}_y$ pointing along y\n",
    "$$\\textbf{u}_y = \\begin{bmatrix} 0 \\\\ \\ 1 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see what happens when we take dot product of $\\nabla f$ with $\\textbf{u}_x$ or $\\textbf{u}_y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\nabla f \\cdot \\textbf{u}_x = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y} \\end{bmatrix} \\begin{bmatrix} 1 \\\\ \\ 0 \\end{bmatrix} $$\n",
    "\n",
    "What is this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\nabla f \\cdot \\textbf{u}_y = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y} \\end{bmatrix} \\begin{bmatrix} 0 \\\\ \\ 1 \\end{bmatrix} $$\n",
    "\n",
    "What is this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The **gradient** encodes local information at $(x_0, y_0)$:\n",
    "- slope along either x and y or in **another direction** \n",
    "- unit vector $\\textbf{u}$ anywhere in x-y plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notion of directional derivative:\n",
    "\n",
    "$$ D_\\textbf{u}(f) = \\nabla{f}\\cdot\\textbf{u} $$\n",
    "\n",
    "- Slope along arbitrary direction $\\textbf{u}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When an applet is worth a thousand equations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "https://mathinsight.org/applet/gradient_directional_derivative_mountain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gradient has an additional interpretation:\n",
    "\n",
    "$$ D_\\textbf{u}(f) = \\nabla{f}\\cdot\\textbf{u} = |\\nabla{f}||\\textbf{u}| \\cos(\\theta) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When $\\textbf{u}$ parallel to $\\nabla{f}$:\n",
    "- $D_\\textbf{u}(f)$ is largest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In essence:\n",
    "- Gradient is vector encoding the magnitude/direction of maximum slope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Or more physically, a skier sampling **gradient** of a landscape:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images\\skiing.jpg\" width = 500 />\n",
    "<center> 1. Be one with the <i>gradient flow</i></center>\n",
    "<center> 2. But do avoid trees. </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Understand the relation of skiing with the mathematical gradient. \n",
    "<img src = \"Images/gradient_anim.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Skiier, at each point, going down direction of maximum slope.\n",
    "- Yes, defined by the mathematical gradient.\n",
    "- Gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Generalizes to arbitrary dimension:\n",
    "\n",
    "- Function\n",
    "$$ f (x_1, x_2, ..., x_N) $$\n",
    "\n",
    "Want \"slope\" of $f$ in given direction of N-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Gradient:\n",
    "\n",
    "$$ \\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1}\\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_N} \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Directional derivative:\n",
    "\n",
    "$$ D_\\textbf{u}(f) = \\nabla{f}\\cdot\\textbf{u} $$\n",
    "\n",
    "- $\\textbf{u}$ now lives in an N-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why am I subjecting you to all this math?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "I enjoying torturing you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But also:\n",
    "    \n",
    "- Conceptual framework of optimization in data science.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Finding minima of loss/cost functions for models with multiple parameters.\n",
    "- Getting optimal weights ($w_0$, $w_1$, $w_2$,...$w_n$) or *fit* parameters for model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "See this in a little more detail in next lecture."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "TOC",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
